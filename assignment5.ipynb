{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.datasets import fetch_openml, load_iris\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "def generate_highly_correlated_data(n_samples=500, n_features=7, rho=0.9, noise_std=1.0):\n",
        "    cov = np.full((n_features, n_features), rho)\n",
        "    np.fill_diagonal(cov, 1.0)\n",
        "    X = np.random.multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
        "    true_beta = np.linspace(1, n_features*0.5, n_features)\n",
        "    y = X.dot(true_beta) + np.random.normal(0, noise_std, size=n_samples)\n",
        "    return X, y\n",
        "\n",
        "def ridge_gradient_descent(X, y, lr, alpha, n_iter=5000):\n",
        "    n, d = X.shape\n",
        "    w = np.zeros(d)\n",
        "    b = 0.0\n",
        "    prev_cost = np.inf\n",
        "\n",
        "    for _ in range(n_iter):\n",
        "        preds = X.dot(w) + b\n",
        "        error = preds - y\n",
        "        cost = (1/(2*n))*np.sum(error**2) + (alpha/2)*np.sum(w**2)\n",
        "\n",
        "        if not np.isfinite(cost) or cost > 1e10:\n",
        "            return None, None, np.inf\n",
        "\n",
        "        grad_w = (1/n)*(X.T.dot(error)) + alpha*w\n",
        "        grad_b = (1/n)*np.sum(error)\n",
        "\n",
        "        w -= lr * grad_w\n",
        "        b -= lr * grad_b\n",
        "\n",
        "\n",
        "        if abs(prev_cost - cost) < 1e-8:\n",
        "            break\n",
        "        prev_cost = cost\n",
        "\n",
        "    return w, b, cost\n",
        "\n",
        "\n",
        "X, y = generate_highly_correlated_data()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_s, X_test_s = scaler.transform(X_train), scaler.transform(X_test)\n",
        "\n",
        "\n",
        "learning_rates = [1e-4, 1e-3, 1e-2, 0.05]\n",
        "alphas = [1e-10, 1e-5, 1e-3, 0.1, 1, 10]\n",
        "\n",
        "results = []\n",
        "for lr in learning_rates:\n",
        "    for alpha in alphas:\n",
        "        w, b, cost = ridge_gradient_descent(X_train_s, y_train, lr, alpha)\n",
        "        if cost == np.inf:\n",
        "            continue\n",
        "        preds = X_test_s.dot(w) + b\n",
        "        r2 = r2_score(y_test, preds)\n",
        "        mse = mean_squared_error(y_test, preds)\n",
        "        results.append({'lr': lr, 'alpha': alpha, 'cost': cost, 'R2': r2, 'MSE': mse})\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "best = df.loc[df['R2'].idxmax()]\n",
        "print(\"\\nQ1: Ridge Regression (Gradient Descent)\")\n",
        "print(df)\n",
        "print(\"\\nBest parameters:\")\n",
        "print(best)\n",
        "\n",
        "\n",
        "\n",
        "boston = fetch_openml(name='Boston', version=1, as_frame=True)\n",
        "\n",
        "\n",
        "Xb = boston.data.copy()\n",
        "yb = boston.target.astype(float)\n",
        "\n",
        "for col in Xb.columns:\n",
        "    if Xb[col].dtype.name == \"category\" or Xb[col].dtype == object:\n",
        "        Xb[col] = pd.to_numeric(Xb[col], errors='coerce')\n",
        "\n",
        "Xb = Xb.fillna(Xb.mean())\n",
        "\n",
        "Xb_train, Xb_test, yb_train, yb_test = train_test_split(Xb, yb, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler_b = StandardScaler().fit(Xb_train)\n",
        "Xb_train_s = scaler_b.transform(Xb_train)\n",
        "Xb_test_s = scaler_b.transform(Xb_test)\n",
        "\n",
        "alphas_cv = np.logspace(-3, 3, 50)\n",
        "\n",
        "ridgecv = RidgeCV(alphas=alphas_cv, store_cv_values=True).fit(Xb_train_s, yb_train)\n",
        "lassocv = LassoCV(alphas=alphas_cv, cv=5, max_iter=10000).fit(Xb_train_s, yb_train)\n",
        "\n",
        "ridge_r2 = r2_score(yb_test, ridgecv.predict(Xb_test_s))\n",
        "lasso_r2 = r2_score(yb_test, lassocv.predict(Xb_test_s))\n",
        "\n",
        "print(\"Q3: RidgeCV & LassoCV (Boston Dataset)\")\n",
        "print(f\"RidgeCV -> Best alpha: {ridgecv.alpha_:.5f}, Test R²: {ridge_r2:.4f}\")\n",
        "print(f\"LassoCV -> Best alpha: {lassocv.alpha_:.5f}, Test R²: {lasso_r2:.4f}\")\n",
        "\n",
        "if ridge_r2 > lasso_r2:\n",
        "    print(\"\\nRidge Regression performs better — it's more stable when features are correlated.\")\n",
        "else:\n",
        "    print(\"\\nLasso Regression performs better — it performs feature selection by shrinking some weights to 0.\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nQ4: Multiclass Logistic Regression (One-vs-Rest)\")\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)\n",
        "\n",
        "def sigmoid(z): return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def train_logistic(X, y, lr=0.1, n_iter=1000):\n",
        "    n, d = X.shape\n",
        "    w, b = np.zeros(d), 0\n",
        "    for _ in range(n_iter):\n",
        "        preds = sigmoid(X.dot(w) + b)\n",
        "        w -= lr * (X.T.dot(preds - y) / n)\n",
        "        b -= lr * np.mean(preds - y)\n",
        "    return w, b\n",
        "\n",
        "classes = np.unique(y_train)\n",
        "weights = {}\n",
        "for c in classes:\n",
        "    y_bin = (y_train == c).astype(float)\n",
        "    w, b = train_logistic(X_train, y_bin)\n",
        "    weights[c] = (w, b)\n",
        "\n",
        "def predict_ovr(X, weights):\n",
        "    scores = np.column_stack([sigmoid(X.dot(w) + b) for w, b in weights.values()])\n",
        "    return np.argmax(scores, axis=1)\n",
        "\n",
        "y_pred = predict_ovr(X_test, weights)\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "print(f\"Accuracy (One-vs-Rest Logistic Regression): {accuracy:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZjOT2P_ieog",
        "outputId": "b0f97a2e-80b3-46c6-8811-8855878900ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q1: Ridge Regression (Gradient Descent)\n",
            "        lr         alpha       cost        R2        MSE\n",
            "0   0.0001  1.000000e-10   1.660622  0.984561   3.086058\n",
            "1   0.0001  1.000000e-05   1.660802  0.984561   3.086074\n",
            "2   0.0001  1.000000e-03   1.678582  0.984553   3.087597\n",
            "3   0.0001  1.000000e-01   3.430182  0.983637   3.270813\n",
            "4   0.0001  1.000000e+00  17.258362  0.964194   7.157198\n",
            "5   0.0001  1.000000e+01  72.467379  0.624297  75.099020\n",
            "6   0.0010  1.000000e-10   0.630606  0.994664   1.066649\n",
            "7   0.0010  1.000000e-05   0.630794  0.994664   1.066652\n",
            "8   0.0010  1.000000e-03   0.649376  0.994662   1.066985\n",
            "9   0.0010  1.000000e-01   2.465418  0.994277   1.143880\n",
            "10  0.0010  1.000000e+00  16.473348  0.975803   4.836793\n",
            "11  0.0010  1.000000e+01  71.750016  0.620788  75.800572\n",
            "12  0.0100  1.000000e-10   0.533332  0.995546   0.890295\n",
            "13  0.0100  1.000000e-05   0.533541  0.995546   0.890293\n",
            "14  0.0100  1.000000e-03   0.554161  0.995547   0.890156\n",
            "15  0.0100  1.000000e-01   2.448603  0.994780   1.043488\n",
            "16  0.0100  1.000000e+00  16.473260  0.975789   4.839500\n",
            "17  0.0100  1.000000e+01  71.749928  0.620676  75.822830\n",
            "18  0.0500  1.000000e-10   0.533273  0.995539   0.891618\n",
            "19  0.0500  1.000000e-05   0.533482  0.995539   0.891614\n",
            "20  0.0500  1.000000e-03   0.554108  0.995541   0.891263\n",
            "21  0.0500  1.000000e-01   2.448601  0.994782   1.042939\n",
            "22  0.0500  1.000000e+00  16.473260  0.975788   4.839639\n",
            "23  0.0500  1.000000e+01  71.749928  0.620671  75.823896\n",
            "\n",
            "Best parameters:\n",
            "lr       0.010000\n",
            "alpha    0.001000\n",
            "cost     0.554161\n",
            "R2       0.995547\n",
            "MSE      0.890156\n",
            "Name: 14, dtype: float64\n",
            "Q3: RidgeCV & LassoCV (Boston Dataset)\n",
            "RidgeCV -> Best alpha: 6.25055, Test R²: 0.6669\n",
            "LassoCV -> Best alpha: 0.00100, Test R²: 0.6687\n",
            "\n",
            "Lasso Regression performs better — it performs feature selection by shrinking some weights to 0.\n",
            "\n",
            "Q4: Multiclass Logistic Regression (One-vs-Rest)\n",
            "Accuracy (One-vs-Rest Logistic Regression): 0.967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1gUOfHp9i45u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}